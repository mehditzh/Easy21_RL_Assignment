# -*- coding: utf-8 -*-
"""easy21_MonteCarlo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OlaU2klESZBzI2H-RoBId1fS23O4ADhm
"""

from easy21_env import easy21
import numpy as np
import random

#Assignment Q2: Monte-Carlo Control in Easy21


class MonteCarlo(object):

    '''I intitialize a 10*21*2*2 array containing the number of each action taken
    from each state (N(s,a)) and the value of q function for each state*action
    (Q(s,a))'''
    def __init__(self):
        self.value_matrix = np.zeros((10, 21, 2, 2))

    def get_value_matrix(self):
        return self.value_matrix

    #this method is used to change the matrix values
    def set_value_matrix(self, first_idx, second_idx, third_idx, fourth_idx,new_value):
        self.value_matrix[first_idx, second_idx, third_idx, fourth_idx] = new_value


    #policy method: returns "action" according to the policy
    def policy(self, state):

        #q values for each action in this state
        stick_q =  self.value_matrix[state[0] - 1, state[1] - 1, 0, 1]
        hit_q =  self.value_matrix[state[0] - 1, state[1] - 1, 1, 1]

        #number of times each action has been picked from this state
        stick_n =  self.value_matrix[state[0] - 1, state[1] - 1, 0, 0]
        hit_n =  self.value_matrix[state[0] - 1, state[1] - 1, 1, 0]

        #number of times this state has been occured
        state_n = hit_n + stick_n

        #initiazlizing parameters used in our epsilon greedy policy
        n_0 = 100
        epsilon = n_0 / (n_0 + state_n)

        #probability for choosing the greedy option
        greedy = 1 - (epsilon/2)

        #probability for choosing the other option
        e_greedy = epsilon/2

        action_pi = random.uniform(0, 1)

        if hit_q == stick_q:
            action = random.randint(0,1)

        #pick the greedy action with probability (1 - epsilon/2)
        elif hit_q > stick_q:
            if action_pi > e_greedy:
                action = 1
            else:
                action = 0

        elif hit_q < stick_q:
            if action_pi > e_greedy:
                action = 0
            else:
                action = 1

        return action


    #this method is used to update value function values in the matrix
    def update_value(self, state, action, return_value):
        q_value = self.value_matrix[state[0]-1, state[1]-1, action, 1]
        action_n = self.value_matrix[state[0]-1, state[1]-1, action, 0]
        q_value = q_value + (return_value - q_value)/action_n
        self.set_value_matrix(state[0]-1, state[1]-1, action, 1, q_value)

    #this method is used to update the state numbers in the matrix
    def update_n(self, state, action):
        n_value = self.value_matrix[state[0]-1, state[1]-1, action, 0] + 1
        self.set_value_matrix(state[0]-1, state[1]-1, action, 0, n_value)

    '''this method is the full process of one episode of easy21 game
    has no return value, does all the necessary updates in our value matrix'''
    def play_episode(self):

        '''updated_states is list which contains all the (state, action) tuples
        that occured in the episode and is used at the end of the episode to
        calculate and update the value function for all these pairs'''
        updated_states = []
        game = easy21()
        state = game.deal_cards()
        action = self.policy(state)
        self.update_n(state, action)
        updated_states.append((state, action))

        if action == 1:
            while action == 1:
                state, reward = game.step(1)
                if reward == -1:
                    break
                action = self.policy(state)
                self.update_n(state, action)
                updated_states.append((state, action))
            if reward != -1:
                state, reward = game.step(0)

        else:
            state, reward = game.step(0)

        for update in updated_states:
            self.update_value(update[0], update[1], reward)

    '''this method runs easy21 episodes the number of times I want
    it takes a number "times" as an input that tells it how many times
    I want to run full episode for my MC control learning'''
    def MC_learning(self, times):
        i = 0
        while i < times:
            self.play_episode()
            i += 1

    '''this method returns a 10*21 array that contains only optimal functions
    for each state, which is just the sum of optimal values of all actions in
    each state'''
    def optimal_values(self):
        opt_val_func = np.zeros((10,21))
        for d_first in range(0,10):
            for p_sum in range(0,21):
                opt_val_func[d_first, p_sum] = max(self.value_matrix[d_first, p_sum, 0, 1], self.value_matrix[d_first, p_sum, 1, 1])

        return opt_val_func

#build a MC object
control = MonteCarlo()
#run my MC learning model times = 20 million
learn = control.MC_learning(20000000)
#get the optimal value matrix and call it data
data = control.optimal_values()


#create optimal values matrix for next part: TD learning
optimal_matrix = control.get_value_matrix()

np.save('MonteCarlo.Optimal_Values.npy', optimal_matrix)

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

#create the figure and a 3d axis
fig = plt.figure(figsize=[8, 7])
ax = fig.add_subplot(111, projection='3d')

#create the grid of x and y values
x = np.arange(1,data.shape[0]+1)
y = np.arange(1,data.shape[1]+1)
x, y = np.meshgrid(x, y)

#plot the surface
ax.plot_surface(x, y, data.T, cmap='viridis')

#set labels
ax.set_xlabel("Dealer's First Card")
ax.set_ylabel("Player's Sum of Card Values")
ax.set_zlabel('Value Function')

ax.view_init(elev=20)
ax.set_zlim(-0.5, 1)
ax.get_proj = lambda: np.dot(Axes3D.get_proj(ax), np.diag([1.0, 1.0, 0.5, 1]))

xticks = np.arange(1,data.shape[0])
xticks = np.append(xticks, data.shape[0])
ax.set_xticks(xticks)
yticks = np.arange(1,data.shape[1],2)
yticks = np.append(yticks, data.shape[1])
ax.set_yticks(yticks)

#show the plot
plt.title('Value Function')
plt.savefig('MonteCarlo.Value_Function_Plot.png')
plt.show()