# -*- coding: utf-8 -*-
"""easy21_Sarsa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ywtW_iOWTKQLJHSEHVKp3iN-6SpbpUP3
"""

from easy21_env import easy21
import numpy as np
import random
import matplotlib.pyplot as plt

'''
Implement Sarsa(lambda).
Initiate the value functino to zero.
Use the same step-size as q2 and same exploration schedules.
Run the algorithm with parameter values lambda ---> {0, 0.1, 0.2, ..., 1}.

Stop each run after 1000 episodes and report the MSE over all states and actions.
comparing the true values computed in the previous section with the estimated Q(s,a) computed by Sarsa.

Plot the MSE against lambda.
For lambda = 0 and lambda = 1 only, plot the learning curve of MSE against episode number.
'''

# mccontrol = MonteCarlo()
# mccontrol.MC_learning(20000000)
# data = mccontrol.optimal_values()


class Sarsa(object):
    def __init__(self):
        self.value_matrix = np.zeros((10, 21, 2, 2))
        self.mse_episodes = np.zeros((10000))
    def get_value_matrix(self):
        return self.value_matrix

    def get_episodes_mse(self):
        return self.mse_episodes

    # this method is used to change the matrix values
    def set_value_matrix(self, first_idx, second_idx, third_idx, fourth_idx, new_value):
        self.value_matrix[first_idx, second_idx, third_idx, fourth_idx] = new_value

    def add_episode_mse(self, i, new_mse):
        self.mse_episodes[i] = new_mse
    # policy method: returns "action" according to the policy
    def policy(self, state):

        # q values for each action in this state
        stick_q = self.value_matrix[state[0] - 1, state[1] - 1, 0, 1]
        hit_q   = self.value_matrix[state[0] - 1, state[1] - 1, 1, 1]

        # number of times each action has been picked from this state
        stick_n = self.value_matrix[state[0] - 1, state[1] - 1, 0, 0]
        hit_n   = self.value_matrix[state[0] - 1, state[1] - 1, 1, 0]

        # number of times this state has been occured
        state_n = hit_n + stick_n

        # initiazlizing parameters used in our policy
        n_0 = 100
        epsilon = n_0 / (n_0 + state_n)

        # probability for choosing the greedy option
        greedy = 1 - (epsilon / 2)
        # probability for choosing the other option
        e_greedy = epsilon / 2

        action_pi = random.uniform(0, 1)

        if hit_q == stick_q:
            action = random.randint(0, 1)

        elif hit_q > stick_q:
            if action_pi > e_greedy:
                action = 1
            else:
                action = 0

        elif hit_q < stick_q:
            if action_pi > e_greedy:
                action = 0
            else:
                action = 1

        return action


    # this method is used to update value function values in the matrix
    def update_value(self, td_error, eligibility):
        for d_first in range(0, 10):
            for p_sum in range(0, 21):
                for action in range(0, 2):
                    if self.value_matrix[d_first, p_sum, action, 0] > 0:
                        #print(f"N(S,A) is not 0, N(S,A) = {self.value_matrix[d_first, p_sum, action, 0]}")
                        alpha = 1 / self.value_matrix[d_first, p_sum, action, 0]

                        #print(f"alpha value is nan: {np.isnan(alpha)}")
                        new_value = (self.value_matrix[d_first, p_sum, action, 1]
                                     + (alpha * td_error * eligibility[d_first, p_sum, action]))

                        self.set_value_matrix(d_first, p_sum, action, 1, new_value)

    def update_eligibility(self, eligibility, td_lambda):
        for d_first in range(0, 10):
            for p_sum in range(0, 21):
                for action in range(0, 2):
                    eligibility[d_first, p_sum, action] = td_lambda * eligibility[d_first, p_sum, action]

        return eligibility

    # this method is used to update the state numbers in the matrix
    def update_n(self, state, action):
        n_value = self.value_matrix[state[0] - 1, state[1] - 1, action, 0] + 1
        self.set_value_matrix(state[0] - 1, state[1] - 1, action, 0, n_value)


    def calculate_tderror(self, reward, next_state, next_action, state, action):
        td_error = (reward + self.value_matrix[next_state[0]-1, next_state[1]-1, next_action, 1]
                    - self.value_matrix[state[0]-1, state[1]-1, action, 1])
        return td_error

    def calculate_ep_mse(self, i, optimal_values):
        this_mse = 0
        num_data_points = 0
        for d_first in range(0, 10):
            for p_sum in range(0, 21):
                for action in range(0, 2):
                    this_mse += ((self.value_matrix[d_first, p_sum, action, 1]
                                 - optimal_values[d_first, p_sum, action, 1]) ** 2)
                    num_data_points += 1
        self.add_episode_mse(i, this_mse/num_data_points)


    def play_episode(self, td_lambda):
        eligibility_matrix = np.zeros((10, 21, 2))
        game = easy21()
        # 1  intiate states
        state = game.deal_cards()
        # 2  choose action
        # action variable assigned a value
        action = self.policy(state)
        # 3   update N(s,a)
        self.update_n(state, action)


        # using action
        if action == 1:

            while action == 1:
                # 5  take action A, observe S' and R
                '''name S' as new_state'''
                next_state, reward = game.step(1)
                if reward == -1:
                    # 6  calculate TD-error
                    td_error = reward - self.value_matrix[state[0]-1,  state[1]-1, action, 1]
                    # 7  increment state,action eligibility
                    eligibility_matrix[state[0]-1, state[1]-1, action] += 1
                    # 8  update value function Q(s,a) for all s,a
                    self.update_value(td_error, eligibility_matrix)
                    # 9  update eligibility matrix E(s,a) for all s,a
                    '''assign the update_eligibility value to our eligibitliy matrix'''
                    eligibility_matrix = self.update_eligibility(eligibility_matrix, td_lambda)



                    break
                # 6  choose A' from S'
                '''next_action variable instead of re-assigning action'''
                '''policy get next_state as input instead of state'''
                next_action = self.policy(next_state)

                # use action
                '''use next_state and next_action'''
                td_error = self.calculate_tderror(reward, next_state, next_action, state, action)
                '''use state, action instead of prev_state list'''
                eligibility_matrix[state[0]-1, state[1]-1, action] += 1

                ''' S<---S', A<---A' '''
                state = next_state
                action = next_action
                '''print(f"---> S < S': {state}"
                      f"---> A < A': {action}")'''
                self.update_n(state, action)

                '''I moved these two lines from above "S<---S' , A<---A' " '''
                '''prev_state will be omitted'''
                self.update_value(td_error, eligibility_matrix)
                '''assign the update_eligibility value to our eligibitliy matrix'''
                eligibility_matrix = self.update_eligibility(eligibility_matrix, td_lambda)

            if reward != -1:
                #action = 0
                '''did not changed state to next_state'''
                state, reward = game.step(0)
                td_error = reward - self.value_matrix[state[0] - 1, state[1] - 1, action, 1]
                eligibility_matrix[state[0] - 1, state[1] - 1, action] += 1
                self.update_value(td_error, eligibility_matrix)

                '''assign the update_eligibility value to our eligibitliy matrix'''
                eligibility_matrix = self.update_eligibility(eligibility_matrix, td_lambda)


        else:
            #action = 0
            '''did not changed state to next_state'''
            state, reward = game.step(0)
            td_error = reward - self.value_matrix[state[0] - 1, state[1] - 1, action, 1]
            eligibility_matrix[state[0] - 1, state[1] - 1, action] += 1
            self.update_value(td_error, eligibility_matrix)

            '''assign the update_eligibility value to our eligibitliy matrix'''
            eligibility_matrix = self.update_eligibility(eligibility_matrix, td_lambda)


    def TD_learning(self, td_lambda, optimal_values):
        i = 0
        while i < 10000:
            self.play_episode(td_lambda)
            self.calculate_ep_mse(i, optimal_values)
            i += 1

    def calculate_mse(self, optimal_values):
        mse_value = 0
        num_data_points = 0
        for d_first in range(0, 10):
            for p_sum in range(0, 21):
                for action in range(0, 2):
                    mse_value += ((self.value_matrix[d_first, p_sum, action, 1] - optimal_values[
                        d_first, p_sum, action, 1]) ** 2)
                    num_data_points += 1

        return mse_value/num_data_points

    def optimal_values(self):
        opt_val_func = np.zeros((10,21))
        for d_first in range(0,10):
            for p_sum in range(0,21):
                opt_val_func[d_first, p_sum] = max(self.value_matrix[d_first, p_sum, 0, 1], self.value_matrix[d_first, p_sum, 1, 1])

        return opt_val_func

optimal_matrix = np.load('MC_values.npy')

td_lambda = 0
mse_lambdas = []

td_control = Sarsa()
td_control.TD_learning(td_lambda, optimal_matrix)
mse_lambdas.append(td_control.calculate_mse(optimal_matrix))
mse_episodes = td_control.get_episodes_mse()




episodes = range(1, 10001)

plt.figure(figsize=(15,6))
plt.plot(episodes, mse_episodes)
plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curve: MSE vs. Episode Number (Lambda = 0)')
plt.grid(True)
plt.show()

lambda_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
for td_lambda in lambda_list:
  td_control = Sarsa()
  td_control.TD_learning(td_lambda, optimal_matrix)
  mse_lambdas.append(td_control.calculate_mse(optimal_matrix))

td_lambda = 1

td_control = Sarsa()
td_control.TD_learning(td_lambda, optimal_matrix)
mse_lambdas.append(td_control.calculate_mse(optimal_matrix))
mse_episodes_1 = td_control.get_episodes_mse()



episodes = range(1, 10001)

plt.figure(figsize=(15,6))
plt.plot(episodes, mse_episodes_1, color='red')
plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curve: MSE vs. Episode Number (Lambda = 1)')
plt.grid(True)
episodes = range(1, 10001)


plt.plot(episodes, mse_episodes, label='Lambda = 0')
plt.plot(episodes, mse_episodes_1, label='Lambda = 1')

plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curves for Different Lambda Values')
plt.grid(True)
plt.legend()
plt.savefig('Sarsa.Leaning_Curves.png')
plt.show()

lambda_values = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]


plt.figure(figsize=(15,6))
plt.plot(lambda_values, mse_lambdas, marker='o')
plt.xlabel('Lambda')
plt.ylabel('MSE')
plt.title('MSE vs. Lambda')
plt.grid(True)
plt.xticks(lambda_values)
plt.savefig('Sarsa.MSE_vs_Lambda.png')
plt.show()