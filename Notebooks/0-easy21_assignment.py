# -*- coding: utf-8 -*-
"""easy21_assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NQMlZhQOUrBKu6uurTbCiS0Btcb3ohKj

# Assignment Q1: Write an Environment to Implement the Game Easy21
"""

#Assignment Q1: Write an Environment to Implement the Game Easy21


"""
define a function "step" which takes an inpute "s" and an action "a",
and returns a sample of the next s' and reward r.
"""
#state s: dealers first card 1-10 and players sum 1-21
#action a: hit or stick


import random
class easy21(object):

    '''initiazlize useful attributes(variables):

       d_first:   Dealer's first card value which is shown to the player
       p_sum:     Sum of player's cards values
       d_sum:     Sum of dealer's cards values
       s:         state of the game ---> [dealer's first card, player's sum]'''
    def __init__(self):
        self.d_first = 0
        self.p_sum = 0
        self.d_sum = 0
        self.s = [self.d_first, self.p_sum]

    def get_p_sum(self):
        return self.p_sum
    def get_d_sum(self):
        return self.d_sum
    def get_d_first(self):
        return self.d_first
    def get_s(self):
        return self.s

    def set_p_sum(self, new_p_card):
        self.p_sum += new_p_card
    def set_d_sum(self, new_d_card):
        self.d_sum += new_d_card
    def set_d_first(self, new_d_first):
        self.d_first = new_d_first
    def set_s(self, dealer_first, player_sum):
        self.s = [dealer_first, player_sum]

    '''this method deals the first cards to the player and dealer
    and returns the state of the game'''
    def deal_cards(self):
        card_vals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        self.set_d_first(card_vals[random.randint(0,9)])
        self.set_d_sum(self.d_first)
        self.set_p_sum(card_vals[random.randint(0,9)])
        self.set_s(self.d_first, self.p_sum)
        return self.s

    '''this method tells us if one of the dealer or player has gone bust
    meaning the sum of their cards is out of the range 1-21'''
    def goes_bust(self):
        if self.d_sum > 21 or self.d_sum < 1:
            return True
        elif self.p_sum > 21 or self.p_sum < 1:
            return True
        else:
            return False

    #this method is used to draw a new card from the pile for the player or dealer
    #returns the change it makes to the sum of card values
    def hit(self):

        #black cards with + and red cards with - impact on sum of values
        b_cards = [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10)]
        r_cards = [(-1, 1), (-1, 2), (-1, 3), (-1, 4), (-1, 5), (-1, 6), (-1, 7), (-1, 8), (-1, 9), (-1, 10)]

        select_color = random.uniform(0, 1)
        select_num = random.randint(0, 9)

        #probability of drawing black cards is 2/3 and red is 1/3
        if select_color > (2/3):
            card = r_cards[select_num]
        else:
            card = b_cards[select_num]

        return card[0] * card[1]

    '''method step takes the action and state as input
    and returns the result state and reward'''
    def step(self, action):
        new_card=0
        #stick = 0 and hit = 1
        reward = 0
        if action == 0:
            dealer_sum = [self.d_sum]

            '''if player chooses to stick, the dealers keeps to hit as long as
            his/her sum of cards is between 1 and 16'''
            while self.d_sum > 0 and self.d_sum < 17:
                self.set_d_sum(self.hit())
                dealer_sum.append(self.d_sum)

            if self.goes_bust():
                reward = 1
            elif self.p_sum > self.d_sum:
                reward = 1
            elif self.p_sum < self.d_sum:
                reward = -1

        else:
            new_card = self.hit()
            self.set_p_sum(new_card)
            if self.goes_bust():
                reward = -1

        self.set_s(self.d_first, self.p_sum)

        return self.s, reward

"""# Assignment Q2: Monte-Carlo Control in Easy21"""

#Assignment Q2: Monte-Carlo Control in Easy21


import numpy as np
import random


class MonteCarlo(object):

    '''I intitialize a 10*21*2*2 array containing the number of each action taken
    from each state (N(s,a)) and the value of q function for each state*action
    (Q(s,a))'''
    def __init__(self):
        self.value_matrix = np.zeros((10, 21, 2, 2))

    def get_value_matrix(self):
        return self.value_matrix

    #this method is used to change the matrix values
    def set_value_matrix(self, first_idx, second_idx, third_idx, fourth_idx,new_value):
        self.value_matrix[first_idx, second_idx, third_idx, fourth_idx] = new_value


    #policy method: returns "action" according to the policy
    def policy(self, state):

        #q values for each action in this state
        stick_q =  self.value_matrix[state[0] - 1, state[1] - 1, 0, 1]
        hit_q =  self.value_matrix[state[0] - 1, state[1] - 1, 1, 1]

        #number of times each action has been picked from this state
        stick_n =  self.value_matrix[state[0] - 1, state[1] - 1, 0, 0]
        hit_n =  self.value_matrix[state[0] - 1, state[1] - 1, 1, 0]

        #number of times this state has been occured
        state_n = hit_n + stick_n

        #initiazlizing parameters used in our epsilon greedy policy
        n_0 = 100
        epsilon = n_0 / (n_0 + state_n)

        #probability for choosing the greedy option
        greedy = 1 - (epsilon/2)

        #probability for choosing the other option
        e_greedy = epsilon/2

        action_pi = random.uniform(0, 1)

        if hit_q == stick_q:
            action = random.randint(0,1)

        #pick the greedy action with probability (1 - epsilon/2)
        elif hit_q > stick_q:
            if action_pi > e_greedy:
                action = 1
            else:
                action = 0

        elif hit_q < stick_q:
            if action_pi > e_greedy:
                action = 0
            else:
                action = 1

        return action


    #this method is used to update value function values in the matrix
    def update_value(self, state, action, return_value):
        q_value = self.value_matrix[state[0]-1, state[1]-1, action, 1]
        action_n = self.value_matrix[state[0]-1, state[1]-1, action, 0]
        q_value = q_value + (return_value - q_value)/action_n
        self.set_value_matrix(state[0]-1, state[1]-1, action, 1, q_value)

    #this method is used to update the state numbers in the matrix
    def update_n(self, state, action):
        n_value = self.value_matrix[state[0]-1, state[1]-1, action, 0] + 1
        self.set_value_matrix(state[0]-1, state[1]-1, action, 0, n_value)

    '''this method is the full process of one episode of easy21 game
    has no return value, does all the necessary updates in our value matrix'''
    def play_episode(self):

        '''updated_states is list which contains all the (state, action) tuples
        that occured in the episode and is used at the end of the episode to
        calculate and update the value function for all these pairs'''
        updated_states = []
        game = easy21()
        state = game.deal_cards()
        action = self.policy(state)
        self.update_n(state, action)
        updated_states.append((state, action))

        if action == 1:
            while action == 1:
                state, reward = game.step(1)
                if reward == -1:
                    break
                action = self.policy(state)
                self.update_n(state, action)
                updated_states.append((state, action))
            if reward != -1:
                state, reward = game.step(0)

        else:
            state, reward = game.step(0)

        for update in updated_states:
            self.update_value(update[0], update[1], reward)

    '''this method runs easy21 episodes the number of times I want
    it takes a number "times" as an input that tells it how many times
    I want to run full episode for my MC control learning'''
    def MC_learning(self, times):
        i = 0
        while i < times:
            self.play_episode()
            i += 1

    '''this method returns a 10*21 array that contains only optimal functions
    for each state, which is just the sum of optimal values of all actions in
    each state'''
    def optimal_values(self):
        opt_val_func = np.zeros((10,21))
        for d_first in range(0,10):
            for p_sum in range(0,21):
                opt_val_func[d_first, p_sum] = max(self.value_matrix[d_first, p_sum, 0, 1], self.value_matrix[d_first, p_sum, 1, 1])

        return opt_val_func

#build a MC object
control = MonteCarlo()
#run my MC learning model times = 20 million
learn = control.MC_learning(20000000)
#get the optimal value matrix and call it data
data = control.optimal_values()


#create optimal values matrix for next part: TD learning
optimal_matrix = control.get_value_matrix()

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

#create the figure and a 3d axis
fig = plt.figure(figsize=[8, 7])
ax = fig.add_subplot(111, projection='3d')

#create the grid of x and y values
x = np.arange(1,data.shape[0]+1)
y = np.arange(1,data.shape[1]+1)
x, y = np.meshgrid(x, y)

#plot the surface
ax.plot_surface(x, y, data.T, cmap='viridis')

#set labels
ax.set_xlabel("Dealer's First Card")
ax.set_ylabel("Player's Sum of Card Values")
ax.set_zlabel('Value Function')

ax.view_init(elev=20)
ax.set_zlim(-0.5, 1)
ax.get_proj = lambda: np.dot(Axes3D.get_proj(ax), np.diag([1.0, 1.0, 0.5, 1]))

xticks = np.arange(1,data.shape[0])
xticks = np.append(xticks, data.shape[0])
ax.set_xticks(xticks)
yticks = np.arange(1,data.shape[1],2)
yticks = np.append(yticks, data.shape[1])
ax.set_yticks(yticks)

#show the plot
plt.show()

"""# Assignment Q3: TD Learning in Easy21"""

#Assignment Q3: TD Learning in Easy21


'''
Implement Sarsa(lambda).
Initiate the value functino to zero.
Use the same step-size as q2 and same exploration schedules.
Run the algorithm with parameter values lambda ---> {0, 0.1, 0.2, ..., 1}.

Stop each run after 1000 episodes and report the MSE over all states and actions.
comparing the true values computed in the previous section with the estimated Q(s,a) computed by Sarsa.

Plot the MSE against lambda.
For lambda = 0 and lambda = 1 only, plot the learning curve of MSE against episode number.
'''

# mccontrol = MonteCarlo()
# mccontrol.MC_learning(20000000)
# data = mccontrol.optimal_values()


class Sarsa(object):
    def __init__(self):
        self.value_matrix = np.zeros((10, 21, 2, 2))
        self.mse_episodes = np.zeros((10000))
    def get_value_matrix(self):
        return self.value_matrix

    def get_episodes_mse(self):
        return self.mse_episodes

    # this method is used to change the matrix values
    def set_value_matrix(self, first_idx, second_idx, third_idx, fourth_idx, new_value):
        self.value_matrix[first_idx, second_idx, third_idx, fourth_idx] = new_value

    def add_episode_mse(self, i, new_mse):
        self.mse_episodes[i] = new_mse
    # policy method: returns "action" according to the policy
    def policy(self, state):

        # q values for each action in this state
        stick_q = self.value_matrix[state[0] - 1, state[1] - 1, 0, 1]
        hit_q   = self.value_matrix[state[0] - 1, state[1] - 1, 1, 1]

        # number of times each action has been picked from this state
        stick_n = self.value_matrix[state[0] - 1, state[1] - 1, 0, 0]
        hit_n   = self.value_matrix[state[0] - 1, state[1] - 1, 1, 0]

        # number of times this state has been occured
        state_n = hit_n + stick_n

        # initiazlizing parameters used in our policy
        n_0 = 100
        epsilon = n_0 / (n_0 + state_n)

        # probability for choosing the greedy option
        greedy = 1 - (epsilon / 2)
        # probability for choosing the other option
        e_greedy = epsilon / 2

        action_pi = random.uniform(0, 1)

        if hit_q == stick_q:
            action = random.randint(0, 1)

        elif hit_q > stick_q:
            if action_pi > e_greedy:
                action = 1
            else:
                action = 0

        elif hit_q < stick_q:
            if action_pi > e_greedy:
                action = 0
            else:
                action = 1

        return action


    # this method is used to update value function values in the matrix
    def update_value(self, td_error, eligibility):
        for d_first in range(0, 10):
            for p_sum in range(0, 21):
                for action in range(0, 2):
                    if self.value_matrix[d_first, p_sum, action, 0] > 0:
                        #print(f"N(S,A) is not 0, N(S,A) = {self.value_matrix[d_first, p_sum, action, 0]}")
                        alpha = 1 / self.value_matrix[d_first, p_sum, action, 0]

                        #print(f"alpha value is nan: {np.isnan(alpha)}")
                        new_value = (self.value_matrix[d_first, p_sum, action, 1]
                                     + (alpha * td_error * eligibility[d_first, p_sum, action]))

                        self.set_value_matrix(d_first, p_sum, action, 1, new_value)

    def update_eligibility(self, eligibility, td_lambda):
        for d_first in range(0, 10):
            for p_sum in range(0, 21):
                for action in range(0, 2):
                    eligibility[d_first, p_sum, action] = td_lambda * eligibility[d_first, p_sum, action]
        return eligibility

    # this method is used to update the state numbers in the matrix
    def update_n(self, state, action):
        n_value = self.value_matrix[state[0] - 1, state[1] - 1, action, 0] + 1
        self.set_value_matrix(state[0] - 1, state[1] - 1, action, 0, n_value)

    def calculate_tderror(self, reward, next_state, next_action, state, action):
        td_error = (reward + self.value_matrix[next_state[0]-1, next_state[1]-1, next_action, 1]
                    - self.value_matrix[state[0]-1, state[1]-1, action, 1])
        return td_error

    def calculate_ep_mse(self, i, optimal_values):
        this_mse = 0
        num_data_points = 0
        for d_first in range(0, 10):
            for p_sum in range(0, 21):
                for action in range(0, 2):
                    this_mse += ((self.value_matrix[d_first, p_sum, action, 1]
                                 - optimal_values[d_first, p_sum, action, 1]) ** 2)
                    num_data_points += 1
        self.add_episode_mse(i, this_mse/num_data_points)


    def play_episode(self, td_lambda):
        eligibility_matrix = np.zeros((10, 21, 2))
        game = easy21()
        # 1  intiate states
        state = game.deal_cards()
        # 2  choose action
        # action variable assigned a value
        action = self.policy(state)
        # 3   update N(s,a)
        # using action variable
        self.update_n(state, action)

        # using action
        if action == 1:

            while action == 1:
                # 5  take action A, observe S' and R
                '''name S' as new_state'''
                next_state, reward = game.step(1)
                if reward == -1:
                    # 6  calculate TD-error
                    '''used state instead of prev_state[0]'''
                    td_error = reward - self.value_matrix[state[0]-1,  state[1]-1, action, 1]
                    # 7  increment state,action eligibility
                    '''used state instead of prev_state[0]
                    no new_action defined yet'''
                    eligibility_matrix[state[0]-1, state[1]-1, action] += 1
                    # 8  update value function Q(s,a) for all s,a
                    self.update_value(td_error, eligibility_matrix)
                    # 9  update eligibility matrix E(s,a) for all s,a
                    '''assign the update_eligibility value to our eligibitliy matrix'''
                    eligibility_matrix = self.update_eligibility(eligibility_matrix, td_lambda)



                    break
                # 6  choose A' from S'
                '''next_action variable instead of re-assigning action'''
                '''policy get next_state as input instead of state'''
                next_action = self.policy(next_state)
                # use action
                '''use next_state and next_action'''
                td_error = self.calculate_tderror(reward, next_state, next_action, state, action)
                '''use state, action instead of prev_state list'''
                eligibility_matrix[state[0]-1, state[1]-1, action] += 1

                ''' S<---S', A<---A' '''
                state = next_state
                action = next_action

                self.update_n(state, action)

                '''I moved these two lines from above "S<---S' , A<---A' " '''
                '''prev_state will be omitted'''
                self.update_value(td_error, eligibility_matrix)
                '''assign the update_eligibility value to our eligibitliy matrix'''
                eligibility_matrix = self.update_eligibility(eligibility_matrix, td_lambda)

            if reward != -1:
                #action = 0
                '''did not changed state to next_state'''
                state, reward = game.step(0)
                td_error = reward - self.value_matrix[state[0] - 1, state[1] - 1, action, 1]
                eligibility_matrix[state[0] - 1, state[1] - 1, action] += 1
                self.update_value(td_error, eligibility_matrix)
                '''assign the update_eligibility value to our eligibitliy matrix'''
                eligibility_matrix = self.update_eligibility(eligibility_matrix, td_lambda)

        else:
            #action = 0
            '''did not changed state to next_state'''
            state, reward = game.step(0)

            td_error = reward - self.value_matrix[state[0] - 1, state[1] - 1, action, 1]
            eligibility_matrix[state[0] - 1, state[1] - 1, action] += 1
            self.update_value(td_error, eligibility_matrix)
            '''assign the update_eligibility value to our eligibitliy matrix'''
            eligibility_matrix = self.update_eligibility(eligibility_matrix, td_lambda)

    def TD_learning(self, td_lambda, optimal_values):
        i = 0
        while i < 10000:
            self.play_episode(td_lambda)
            self.calculate_ep_mse(i, optimal_values)
            i += 1

    def calculate_mse(self, optimal_values):
        mse_value = 0
        num_data_points = 0
        for d_first in range(0, 10):
            for p_sum in range(0, 21):
                for action in range(0, 2):
                    mse_value += ((self.value_matrix[d_first, p_sum, action, 1] - optimal_values[
                        d_first, p_sum, action, 1]) ** 2)
                    num_data_points += 1

        return mse_value/num_data_points

    def optimal_values(self):
        opt_val_func = np.zeros((10,21))
        for d_first in range(0,10):
            for p_sum in range(0,21):
                opt_val_func[d_first, p_sum] = max(self.value_matrix[d_first, p_sum, 0, 1], self.value_matrix[d_first, p_sum, 1, 1])

        return opt_val_func

td_lambda = 0
mse_lambdas = []

td_control = Sarsa()
td_control.TD_learning(td_lambda, optimal_matrix)
mse_lambdas.append(td_control.calculate_mse(optimal_matrix))
mse_episodes = td_control.get_episodes_mse()



episodes = range(1, 10001)

plt.figure(figsize=(15,6))
plt.plot(episodes, mse_episodes)
plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curve: MSE vs. Episode Number (Lambda = 0)')
plt.grid(True)
plt.show()

lambda_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
for td_lambda in lambda_list:
  td_control = Sarsa()
  td_control.TD_learning(td_lambda, optimal_matrix)
  mse_lambdas.append(td_control.calculate_mse(optimal_matrix))

td_lambda = 1

td_control = Sarsa()
td_control.TD_learning(td_lambda, optimal_matrix)
mse_lambdas.append(td_control.calculate_mse(optimal_matrix))
mse_episodes_1 = td_control.get_episodes_mse()


episodes = range(1, 10001)

plt.figure(figsize=(15,6))
plt.plot(episodes, mse_episodes_1, color='red')
plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curve: MSE vs. Episode Number (Lambda = 1)')
plt.grid(True)
plt.show()

lambda_values = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]

plt.figure(figsize=(15,6))
plt.plot(lambda_values, mse_lambdas, marker='o')
plt.xlabel('Lambda')
plt.ylabel('MSE')
plt.title('MSE vs. Lambda')
plt.grid(True)
plt.xticks(lambda_values)
plt.show()

episodes = range(1, 10001)

plt.figure(figsize=(15,6))
plt.plot(episodes, mse_episodes, label='Lambda = 0')
plt.plot(episodes, mse_episodes_1, label='Lambda = 1')

plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curves for Different Lambda Values')
plt.grid(True)
plt.legend()
plt.show()

lambdas = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
mse_episodes = np.zeros((len(lambdas), 10000))
mse_lambdas = []
index = 0
for td_lambda in lambdas:
  td_control = Sarsa()
  td_control.TD_learning(td_lambda, optimal_matrix)
  mse_lambdas.append(td_control.calculate_mse(optimal_matrix))
  mse_episodes[index] = td_control.get_episodes_mse()
  index += 1

import itertools
linestyles = ['-', '--', '-.', ':']
colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']

linestyle_cycle = itertools.cycle(linestyles)
color_cycle = itertools.cycle(colors)

episodes = range(1, 10001)

plt.figure(figsize=(15,6))
plt.plot(episodes, mse_episodes[0], label='Lambda = 0')
plt.plot(episodes, mse_episodes[1], label='Lambda = 0.1')
plt.plot(episodes, mse_episodes[2], label='Lambda = 0.2')
plt.plot(episodes, mse_episodes[3], label='Lambda = 0.3')
plt.plot(episodes, mse_episodes[4], label='Lambda = 0.4')
plt.plot(episodes, mse_episodes[5], label='Lambda = 0.5')
plt.plot(episodes, mse_episodes[6], label='Lambda = 0.6')
plt.plot(episodes, mse_episodes[7], label='Lambda = 0.7')
plt.plot(episodes, mse_episodes[8], label='Lambda = 0.8')
plt.plot(episodes, mse_episodes[9], label='Lambda = 0.9')
plt.plot(episodes, mse_episodes[10], label='Lambda = 1')

plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curves for All Lambda Values')
plt.grid(True)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

plt.figure(figsize=(15,6))
plt.plot(lambdas, mse_lambdas, marker='o')
plt.xlabel('Lambda')
plt.ylabel('MSE')
plt.title('MSE vs. Lambda (10K episodes - training #1)')
plt.grid(True)
plt.xticks(lambdas)
plt.show()

"""# Assignment Q4: Linear Function Approximation in Easy21"""

#Assignment Q4: Linear Function Approximation in Easy21

dealer_s = [(1, 4), (4, 7), (7, 10)]
player_s = [(1, 6), (4, 9), (7, 12), (10, 15), (13, 18), (16, 21)]
a        = [0, 1]


class Function_Approximation(object):
    def __init__(self):
        self.value_matrix = np.zeros((10, 21, 2))
        self.episodes_mse = np.zeros((10000))
        self.weights_vector = np.zeros((3,6,2))
        self.feature_vector = np.zeros((3,6,2))

    def get_value_matrix(self):
        return self.value_matrix

    def get_episodes_mse(self):
        return self.episodes_mse

    def set_value_matrix(self, state, action, new_value):
        self.value_matrix[state[0], state[1], action] = new_value

    def add_episode_mse(self, i, new_mse):
        self.episodes_mse[i] = new_mse

    def convert_state(self, state, action):
        self.feature_vector = np.zeros((3, 6, 2))
        if 0 < state[0] < 5:
            if 0 < state[1] < 7:
                if action == 0:
                    self.feature_vector[0, 0, 0] = 1
                elif action == 1:
                    self.feature_vector[0, 0, 1] = 1
            if 3 < state[1] < 10:
                if action == 0:
                    self.feature_vector[0, 1, 0] = 1
                elif action == 1:
                    self.feature_vector[0, 1, 1] = 1
            if 6 < state[1] < 13:
                if action == 0:
                    self.feature_vector[0, 2, 0] = 1
                elif action == 1:
                    self.feature_vector[0, 2, 1] = 1
            if 9 < state[1] < 16:
                if action == 0:
                    self.feature_vector[0, 3, 0] = 1
                elif action == 1:
                    self.feature_vector[0, 3, 1] = 1
            if 12 < state[1] < 19:
                if action == 0:
                    self.feature_vector[0, 4, 0] = 1
                elif action == 1:
                    self.feature_vector[0, 4, 1] = 1
            if 15 < state[1] < 22:
                if action == 0:
                    self.feature_vector[0, 5, 0] = 1
                elif action == 1:
                    self.feature_vector[0, 5, 1] = 1

        if 3 < state[0] < 8:
            if 0 < state[1] < 7:
                if action == 0:
                    self.feature_vector[1, 0, 0] = 1
                elif action == 1:
                    self.feature_vector[1, 0, 1] = 1
            if 3 < state[1] < 10:
                if action == 0:
                    self.feature_vector[1, 1, 0] = 1
                elif action == 1:
                    self.feature_vector[1, 1, 1] = 1
            if 6 < state[1] < 13:
                if action == 0:
                    self.feature_vector[1, 2, 0] = 1
                elif action == 1:
                    self.feature_vector[1, 2, 1] = 1
            if 9 < state[1] < 16:
                if action == 0:
                    self.feature_vector[1, 3, 0] = 1
                elif action == 1:
                    self.feature_vector[1, 3, 1] = 1
            if 12 < state[1] < 19:
                if action == 0:
                    self.feature_vector[1, 4, 0] = 1
                elif action == 1:
                    self.feature_vector[1, 4, 1] = 1
            if 15 < state[1] < 22:
                if action == 0:
                    self.feature_vector[1, 5, 0] = 1
                elif action == 1:
                    self.feature_vector[1, 5, 1] = 1

        if 6 < state[0] < 11:
            if 0 < state[1] < 7:
                if action == 0:
                    self.feature_vector[2, 0, 0] = 1
                elif action == 1:
                    self.feature_vector[2, 0, 1] = 1
            if 3 < state[1] < 10:
                if action == 0:
                    self.feature_vector[2, 1, 0] = 1
                elif action == 1:
                    self.feature_vector[2, 1, 1] = 1
            if 6 < state[1] < 13:
                if action == 0:
                    self.feature_vector[2, 2, 0] = 1
                elif action == 1:
                    self.feature_vector[2, 2, 1] = 1
            if 9 < state[1] < 16:
                if action == 0:
                    self.feature_vector[2, 3, 0] = 1
                elif action == 1:
                    self.feature_vector[2, 3, 1] = 1
            if 12 < state[1] < 19:
                if action == 0:
                    self.feature_vector[2, 4, 0] = 1
                elif action == 1:
                    self.feature_vector[2, 4, 1] = 1
            if 15 < state[1] < 22:
                if action == 0:
                    self.feature_vector[2, 5, 0] = 1
                elif action == 1:
                    self.feature_vector[2, 5, 1] = 1

    def increment_eligibility(self, eligibility_vector):
        indices = np.where(self.feature_vector == 1)
        indices = np.array(indices).transpose()
        for index in indices:
            eligibility_vector[index[0], index[1], index[2]] += 1

    def decrement_eligibility(self, eligibility_vector, td_lambda):
        for d_first in range(0,3):
            for p_sum in range(0,6):
                for a in range(0,2):
                    eligibility_vector[d_first, p_sum, a] = ((td_lambda * eligibility_vector[d_first, p_sum, a])
                                                             + self.feature_vector[d_first, p_sum, a])

    def update_weights(self, td_error, eligibility_vector):
        for d_first in range(0,3):
            for p_sum in range(0,6):
                for a in range(0,2):
                    self.weights_vector[d_first, p_sum, a] += 0.01 * td_error * eligibility_vector[d_first, p_sum, a]

    def policy(self, state):
        epsilon = 0.05
        self.convert_state(state, 0)
        stick_phi = self.feature_vector
        self.convert_state(state, 1)
        hit_phi = self.feature_vector
        stick_q = np.dot(stick_phi.flatten(), self.weights_vector.flatten())
        hit_q = np.dot(hit_phi.flatten(), self.weights_vector.flatten())

        e_greedy = epsilon/2
        action_pi = random.uniform(0,1)

        if hit_q == stick_q:
            action = random.randint(0,1)
        elif hit_q > stick_q:
            if action_pi > e_greedy:
                action = 1
            else:
                action = 0
        elif hit_q < stick_q:
            if action_pi > e_greedy:
                action = 0
            else:
                action = 1

        return action

    def calculate_tderror(self, reward, next_state, next_action, state, action):
        self.convert_state(next_state, next_action)
        next_phi = self.feature_vector
        self.convert_state(state, action)
        current_phi = self.feature_vector
        td_error = (reward + np.dot(next_phi.flatten(), self.weights_vector.flatten())
                    - np.dot(current_phi.flatten(), self.weights_vector.flatten()))

        return td_error

    def calculate_ep_mse(self, i, optimal_values):
        this_mse = 0
        for d_first in range(0, 10):
            for p_sum in range(0, 21):
                for action in range(0, 2):
                    this_mse += ((self.value_matrix[d_first, p_sum, action]
                                 - optimal_values[d_first, p_sum, action, 1]) ** 2)
        self.add_episode_mse(i, (this_mse/420))

    def update_value_matrix(self):
        for d_first in range(0,10):
            for p_sum in range(0,21):
                for a in range(0,2):
                    state = [d_first, p_sum]
                    self.convert_state(state, a)
                    phi = self.feature_vector
                    self.value_matrix[d_first, p_sum, a] = np.dot(phi.flatten(), self.weights_vector.flatten())
    def play_episode(self, td_lambda):
        eligibility_vector = np.zeros((3, 6, 2))
        game = easy21()
        state = game.deal_cards()
        action = random.randint(0,1)
        self.convert_state(state, action)
        if action == 1:
            while action == 1:
                next_state, reward = game.step(1)
                if reward == -1:
                    td_error = reward - np.dot(self.feature_vector.flatten(), self.weights_vector.flatten())
                    self.increment_eligibility(eligibility_vector)
                    self.decrement_eligibility(eligibility_vector, td_lambda)
                    self.update_weights(td_error, eligibility_vector)
                    break
                next_action = self.policy(next_state)
                td_error = self.calculate_tderror(reward, next_state, next_action, state, action)
                '''be careful
                ---> are we using the correct feature vector phi
                for updating eligibility vector and weights vector'''
                self.increment_eligibility(eligibility_vector)
                self.decrement_eligibility(eligibility_vector, td_lambda)
                self.update_weights(td_error, eligibility_vector)
                state = next_state
                action = next_action
                self.convert_state(state, action)

            if reward != -1:
                state, reward = game.step(0)
                td_error = reward - np.dot(self.feature_vector.flatten(), self.weights_vector.flatten())
                self.increment_eligibility(eligibility_vector)
                self.decrement_eligibility(eligibility_vector, td_lambda)
                self.update_weights(td_error, eligibility_vector)

        else:
            state, reward = game.step(0)
            td_error = reward - np.dot(self.feature_vector.flatten(), self.weights_vector.flatten())
            self.increment_eligibility(eligibility_vector)
            self.decrement_eligibility(eligibility_vector, td_lambda)
            self.update_weights(td_error, eligibility_vector)

        self.update_value_matrix()

    def FN_approximation_learning(self, td_lambda, optimal_values):
        i = 0
        while i < 10000:
            self.play_episode(td_lambda)
            self.calculate_ep_mse(i, optimal_values)
            i += 1
    def calculate_mse(self, optimal_values):
        mse_value = 0
        num_data_points = 0
        for d_first in range(0, 10):
            for p_sum in range(0, 21):
                for action in range(0, 2):
                    mse_value += ((self.value_matrix[d_first, p_sum, action] - optimal_values[
                        d_first, p_sum, action, 1]) ** 2)
                    num_data_points += 1

        return mse_value / num_data_points

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


td_lambda = 0
mse_lambdas = []

fn_approximator = Function_Approximation()
fn_approximator.FN_approximation_learning(td_lambda, optimal_matrix)
mse_lambdas.append(fn_approximator.calculate_mse(optimal_matrix))
mse_episodes = fn_approximator.get_episodes_mse()



episodes = range(1, 10001)

plt.figure(figsize=(15,6))
plt.plot(episodes, mse_episodes)
plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curve: MSE vs. Episode Number (Lambda = 0)')
plt.grid(True)
plt.show()

lambda_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
for td_lambda in lambda_list:
  fn_approximator = Function_Approximation()
  fn_approximator.FN_approximation_learning(td_lambda, optimal_matrix)
  mse_lambdas.append(fn_approximator.calculate_mse(optimal_matrix))

td_lambda = 1

fn_approximator = Function_Approximation()
fn_approximator.FN_approximation_learning(td_lambda, optimal_matrix)
mse_lambdas.append(fn_approximator.calculate_mse(optimal_matrix))
mse_episodes_1 = fn_approximator.get_episodes_mse()


episodes = range(1, 10001)

plt.figure(figsize=(15,6))
plt.plot(episodes, mse_episodes_1, color='red')
plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curve: MSE vs. Episode Number (Lambda = 1)')
plt.grid(True)
plt.show()

episodes = range(1, 10001)

plt.figure(figsize=(15,6))
plt.plot(episodes, mse_episodes, label='Lambda = 0')
plt.plot(episodes, mse_episodes_1, label='Lambda = 1')

plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curves for Different Lambda Values')
plt.grid(True)
plt.legend()
plt.show()