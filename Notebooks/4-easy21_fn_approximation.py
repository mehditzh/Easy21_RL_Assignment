# -*- coding: utf-8 -*-
"""easy21_FN_Approximation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dPsVhV1v13nrEfIwcCeKRSrWWXsVQLaU
"""

from easy21_env import easy21
import numpy as np
import random
import matplotlib.pyplot as plt

class Function_Approximation(object):
    def __init__(self):
        self.value_matrix = np.zeros((10, 21, 2))
        self.episodes_mse = np.zeros((10000))
        self.weights_vector = np.zeros((3,6,2))
        self.feature_vector = np.zeros((3,6,2))

    def get_value_matrix(self):
        return self.value_matrix

    def get_episodes_mse(self):
        return self.episodes_mse

    def set_value_matrix(self, state, action, new_value):
        self.value_matrix[state[0], state[1], action] = new_value

    def add_episode_mse(self, i, new_mse):
        self.episodes_mse[i] = new_mse

    def convert_state(self, state, action):
        self.feature_vector = np.zeros((3, 6, 2))
        if 0 < state[0] < 5:
            if 0 < state[1] < 7:
                if action == 0:
                    self.feature_vector[0, 0, 0] = 1
                elif action == 1:
                    self.feature_vector[0, 0, 1] = 1
            if 3 < state[1] < 10:
                if action == 0:
                    self.feature_vector[0, 1, 0] = 1
                elif action == 1:
                    self.feature_vector[0, 1, 1] = 1
            if 6 < state[1] < 13:
                if action == 0:
                    self.feature_vector[0, 2, 0] = 1
                elif action == 1:
                    self.feature_vector[0, 2, 1] = 1
            if 9 < state[1] < 16:
                if action == 0:
                    self.feature_vector[0, 3, 0] = 1
                elif action == 1:
                    self.feature_vector[0, 3, 1] = 1
            if 12 < state[1] < 19:
                if action == 0:
                    self.feature_vector[0, 4, 0] = 1
                elif action == 1:
                    self.feature_vector[0, 4, 1] = 1
            if 15 < state[1] < 22:
                if action == 0:
                    self.feature_vector[0, 5, 0] = 1
                elif action == 1:
                    self.feature_vector[0, 5, 1] = 1

        if 3 < state[0] < 8:
            if 0 < state[1] < 7:
                if action == 0:
                    self.feature_vector[1, 0, 0] = 1
                elif action == 1:
                    self.feature_vector[1, 0, 1] = 1
            if 3 < state[1] < 10:
                if action == 0:
                    self.feature_vector[1, 1, 0] = 1
                elif action == 1:
                    self.feature_vector[1, 1, 1] = 1
            if 6 < state[1] < 13:
                if action == 0:
                    self.feature_vector[1, 2, 0] = 1
                elif action == 1:
                    self.feature_vector[1, 2, 1] = 1
            if 9 < state[1] < 16:
                if action == 0:
                    self.feature_vector[1, 3, 0] = 1
                elif action == 1:
                    self.feature_vector[1, 3, 1] = 1
            if 12 < state[1] < 19:
                if action == 0:
                    self.feature_vector[1, 4, 0] = 1
                elif action == 1:
                    self.feature_vector[1, 4, 1] = 1
            if 15 < state[1] < 22:
                if action == 0:
                    self.feature_vector[1, 5, 0] = 1
                elif action == 1:
                    self.feature_vector[1, 5, 1] = 1

        if 6 < state[0] < 11:
            if 0 < state[1] < 7:
                if action == 0:
                    self.feature_vector[2, 0, 0] = 1
                elif action == 1:
                    self.feature_vector[2, 0, 1] = 1
            if 3 < state[1] < 10:
                if action == 0:
                    self.feature_vector[2, 1, 0] = 1
                elif action == 1:
                    self.feature_vector[2, 1, 1] = 1
            if 6 < state[1] < 13:
                if action == 0:
                    self.feature_vector[2, 2, 0] = 1
                elif action == 1:
                    self.feature_vector[2, 2, 1] = 1
            if 9 < state[1] < 16:
                if action == 0:
                    self.feature_vector[2, 3, 0] = 1
                elif action == 1:
                    self.feature_vector[2, 3, 1] = 1
            if 12 < state[1] < 19:
                if action == 0:
                    self.feature_vector[2, 4, 0] = 1
                elif action == 1:
                    self.feature_vector[2, 4, 1] = 1
            if 15 < state[1] < 22:
                if action == 0:
                    self.feature_vector[2, 5, 0] = 1
                elif action == 1:
                    self.feature_vector[2, 5, 1] = 1

    def increment_eligibility(self, eligibility_vector):
        indices = np.where(self.feature_vector == 1)
        indices = np.array(indices).transpose()
        for index in indices:
            eligibility_vector[index[0], index[1], index[2]] += 1

    def decrement_eligibility(self, eligibility_vector, td_lambda):
        for d_first in range(0,3):
            for p_sum in range(0,6):
                for a in range(0,2):
                    eligibility_vector[d_first, p_sum, a] = ((td_lambda * eligibility_vector[d_first, p_sum, a])
                                                             + self.feature_vector[d_first, p_sum, a])

    def update_weights(self, td_error, eligibility_vector):
        for d_first in range(0,3):
            for p_sum in range(0,6):
                for a in range(0,2):
                    self.weights_vector[d_first, p_sum, a] += 0.01 * td_error * eligibility_vector[d_first, p_sum, a]

    def policy(self, state):
        epsilon = 0.05
        self.convert_state(state, 0)
        stick_phi = self.feature_vector
        self.convert_state(state, 1)
        hit_phi = self.feature_vector
        stick_q = np.dot(stick_phi.flatten(), self.weights_vector.flatten())
        hit_q = np.dot(hit_phi.flatten(), self.weights_vector.flatten())

        e_greedy = epsilon/2
        action_pi = random.uniform(0,1)

        if hit_q == stick_q:
            action = random.randint(0,1)
        elif hit_q > stick_q:
            if action_pi > e_greedy:
                action = 1
            else:
                action = 0
        elif hit_q < stick_q:
            if action_pi > e_greedy:
                action = 0
            else:
                action = 1

        return action

    def calculate_tderror(self, reward, next_state, next_action, state, action):
        self.convert_state(next_state, next_action)
        next_phi = self.feature_vector
        self.convert_state(state, action)
        current_phi = self.feature_vector
        td_error = (reward + np.dot(next_phi.flatten(), self.weights_vector.flatten())
                    - np.dot(current_phi.flatten(), self.weights_vector.flatten()))

        return td_error

    def calculate_ep_mse(self, i, optimal_values):
        this_mse = 0
        for d_first in range(0, 10):
            for p_sum in range(0, 21):
                for action in range(0, 2):
                    this_mse += ((self.value_matrix[d_first, p_sum, action]
                                 - optimal_values[d_first, p_sum, action, 1]) ** 2)
        self.add_episode_mse(i, (this_mse/420))

    def update_value_matrix(self):
        for d_first in range(0,10):
            for p_sum in range(0,21):
                for a in range(0,2):
                    state = [d_first, p_sum]
                    self.convert_state(state, a)
                    phi = self.feature_vector
                    self.value_matrix[d_first, p_sum, a] = np.dot(phi.flatten(), self.weights_vector.flatten())
    def play_episode(self, td_lambda):
        eligibility_vector = np.zeros((3, 6, 2))
        game = easy21()
        state = game.deal_cards()
        action = random.randint(0,1)
        self.convert_state(state, action)
        if action == 1:
            while action == 1:
                next_state, reward = game.step(1)
                if reward == -1:
                    td_error = reward - np.dot(self.feature_vector.flatten(), self.weights_vector.flatten())
                    self.increment_eligibility(eligibility_vector)
                    self.decrement_eligibility(eligibility_vector, td_lambda)
                    self.update_weights(td_error, eligibility_vector)
                    break
                next_action = self.policy(next_state)
                td_error = self.calculate_tderror(reward, next_state, next_action, state, action)
                '''be careful
                ---> are we using the correct feature vector phi
                for updating eligibility vector and weights vector'''
                self.increment_eligibility(eligibility_vector)
                self.decrement_eligibility(eligibility_vector, td_lambda)
                self.update_weights(td_error, eligibility_vector)
                state = next_state
                action = next_action
                self.convert_state(state, action)

            if reward != -1:
                state, reward = game.step(0)
                td_error = reward - np.dot(self.feature_vector.flatten(), self.weights_vector.flatten())
                self.increment_eligibility(eligibility_vector)
                self.decrement_eligibility(eligibility_vector, td_lambda)
                self.update_weights(td_error, eligibility_vector)

        else:
            state, reward = game.step(0)
            td_error = reward - np.dot(self.feature_vector.flatten(), self.weights_vector.flatten())
            self.increment_eligibility(eligibility_vector)
            self.decrement_eligibility(eligibility_vector, td_lambda)
            self.update_weights(td_error, eligibility_vector)

        self.update_value_matrix()

    def FN_approximation_learning(self, td_lambda, optimal_values):
        i = 0
        while i < 10000:
            self.play_episode(td_lambda)
            self.calculate_ep_mse(i, optimal_values)
            i += 1
    def calculate_mse(self, optimal_values):
        mse_value = 0
        num_data_points = 0
        for d_first in range(0, 10):
            for p_sum in range(0, 21):
                for action in range(0, 2):
                    mse_value += ((self.value_matrix[d_first, p_sum, action] - optimal_values[
                        d_first, p_sum, action, 1]) ** 2)
                    num_data_points += 1

        return mse_value / num_data_points

optimal_matrix = np.load('MC_values.npy')

td_lambda = 0
mse_lambdas = []

fn_approximator = Function_Approximation()
fn_approximator.FN_approximation_learning(td_lambda, optimal_matrix)
mse_lambdas.append(fn_approximator.calculate_mse(optimal_matrix))
mse_episodes = fn_approximator.get_episodes_mse()



episodes = range(1, 10001)

plt.figure(figsize=(15,6))
plt.plot(episodes, mse_episodes)
plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curve: MSE vs. Episode Number (Lambda = 0)')
plt.grid(True)
plt.show()

lambda_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
for td_lambda in lambda_list:
  fn_approximator = Function_Approximation()
  fn_approximator.FN_approximation_learning(td_lambda, optimal_matrix)
  mse_lambdas.append(fn_approximator.calculate_mse(optimal_matrix))

td_lambda = 1

fn_approximator = Function_Approximation()
fn_approximator.FN_approximation_learning(td_lambda, optimal_matrix)
mse_lambdas.append(fn_approximator.calculate_mse(optimal_matrix))
mse_episodes_1 = fn_approximator.get_episodes_mse()


episodes = range(1, 10001)

plt.figure(figsize=(15,6))
plt.plot(episodes, mse_episodes_1, color='red')
plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curve: MSE vs. Episode Number (Lambda = 1)')
plt.grid(True)
plt.show()

episodes = range(1, 10001)


plt.figure(figsize=(15,6))
plt.plot(episodes, mse_episodes, label='Lambda = 0')
plt.plot(episodes, mse_episodes_1, label='Lambda = 1')

plt.xlabel('Episode Number')
plt.ylabel('MSE')
plt.title('Learning Curves for Different Lambda Values')
plt.grid(True)
plt.legend()
plt.savefig('FN_Approximator.Learning_Curve.png')
plt.show()

lambda_values = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]

plt.figure(figsize=(15,6))
plt.plot(lambda_values, mse_lambdas, marker='o')
plt.xlabel('Lambda')
plt.ylabel('MSE')
plt.title('MSE vs. Lambda')
plt.grid(True)
plt.xticks(lambda_values)
plt.savefig('FN_Approximator.MSE_vs_lambdas.png')
plt.show()